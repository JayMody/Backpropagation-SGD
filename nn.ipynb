{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (1.26.2)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.11/site-packages (2.16.0)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.11/site-packages (10.1.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./.venv/lib/python3.11/site-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./.venv/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in ./.venv/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.venv/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./.venv/lib/python3.11/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in ./.venv/lib/python3.11/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in ./.venv/lib/python3.11/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.11/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in ./.venv/lib/python3.11/site-packages (from datasets) (0.20.1)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy datasets Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return x / np.sum(x, axis=-1, keepdims=True)\n",
    "\n",
    "def log_softmax(x):\n",
    "    x = x - np.max(x, axis=-1, keepdims=True)\n",
    "    return x - np.log(np.sum(np.exp(x), axis=-1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, y):\n",
    "    return -log_softmax(logits)[y]\n",
    "\n",
    "def d_loss_fn(logits, y):\n",
    "    p = softmax(logits)\n",
    "    p[y] -= 1\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def d_relu(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(in_size: int, out_size: int, hidden_sizes: list[int]):\n",
    "    dims = [in_size] + hidden_sizes + [out_size]\n",
    "    params = []\n",
    "    for nin, nout in zip(dims[:-1], dims[1:]):\n",
    "        # xavier uniform initialization for weight matrixes\n",
    "        a = (6 / (nin + nout)) ** 0.5\n",
    "        w = np.random.uniform(-a, a, size=[nin, nout])\n",
    "        b = np.zeros(nout)\n",
    "        params.append([w, b])\n",
    "    return params\n",
    "\n",
    "def forward(x, params):\n",
    "    tape = [(None, x)]\n",
    "    for w, b in params:\n",
    "        z = x @ w + b\n",
    "        x = relu(z)\n",
    "        tape.append((z, x))\n",
    "    return z, tape[:-1]\n",
    "\n",
    "def backprop(x, y, params):\n",
    "    logits, tape = forward(x, params)\n",
    "\n",
    "    grad = []\n",
    "    error = d_loss_fn(logits, y)\n",
    "    for (z, a), (w, _) in zip(reversed(tape), reversed(params)):\n",
    "        grad.append((error * a.reshape(-1, 1), error))\n",
    "        if z is not None:\n",
    "            error = error @ w.T\n",
    "            error = error * d_relu(z)\n",
    "    grad = list(reversed(grad))\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jay/code/backpropagation/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 | acc: 0.0457\n",
      "step: 100 | acc: 0.0761\n",
      "step: 200 | acc: 0.0822\n",
      "step: 300 | acc: 0.1149\n",
      "step: 400 | acc: 0.1443\n",
      "step: 500 | acc: 0.1772\n",
      "step: 600 | acc: 0.1966\n",
      "step: 700 | acc: 0.2349\n",
      "step: 800 | acc: 0.2684\n",
      "step: 900 | acc: 0.2979\n",
      "step: 1000 | acc: 0.3200\n",
      "step: 1100 | acc: 0.3586\n",
      "step: 1200 | acc: 0.3876\n",
      "step: 1300 | acc: 0.3999\n",
      "step: 1400 | acc: 0.4189\n",
      "step: 1500 | acc: 0.4233\n",
      "step: 1600 | acc: 0.4278\n",
      "step: 1700 | acc: 0.4469\n",
      "step: 1800 | acc: 0.4444\n",
      "step: 1900 | acc: 0.4570\n",
      "step: 2000 | acc: 0.4629\n",
      "step: 2100 | acc: 0.4896\n",
      "step: 2200 | acc: 0.5072\n",
      "step: 2300 | acc: 0.5123\n",
      "step: 2400 | acc: 0.5087\n",
      "step: 2500 | acc: 0.5209\n",
      "step: 2600 | acc: 0.5285\n",
      "step: 2700 | acc: 0.5401\n",
      "step: 2800 | acc: 0.5468\n",
      "step: 2900 | acc: 0.5605\n",
      "step: 3000 | acc: 0.5795\n",
      "step: 3100 | acc: 0.5805\n",
      "step: 3200 | acc: 0.6000\n",
      "step: 3300 | acc: 0.5831\n",
      "step: 3400 | acc: 0.6299\n",
      "step: 3500 | acc: 0.6398\n",
      "step: 3600 | acc: 0.5819\n",
      "step: 3700 | acc: 0.6111\n",
      "step: 3800 | acc: 0.6303\n",
      "step: 3900 | acc: 0.6369\n",
      "step: 4000 | acc: 0.6586\n",
      "step: 4100 | acc: 0.6355\n",
      "step: 4200 | acc: 0.6763\n",
      "step: 4300 | acc: 0.6607\n",
      "step: 4400 | acc: 0.6523\n",
      "step: 4500 | acc: 0.6730\n",
      "step: 4600 | acc: 0.6593\n",
      "step: 4700 | acc: 0.6621\n",
      "step: 4800 | acc: 0.6717\n",
      "step: 4900 | acc: 0.6900\n",
      "step: 5000 | acc: 0.6854\n",
      "step: 5100 | acc: 0.6999\n",
      "step: 5200 | acc: 0.7159\n",
      "step: 5300 | acc: 0.6892\n",
      "step: 5400 | acc: 0.7217\n",
      "step: 5500 | acc: 0.7156\n",
      "step: 5600 | acc: 0.6990\n",
      "step: 5700 | acc: 0.7075\n",
      "step: 5800 | acc: 0.7319\n",
      "step: 5900 | acc: 0.7180\n",
      "step: 6000 | acc: 0.7328\n",
      "step: 6100 | acc: 0.7100\n",
      "step: 6200 | acc: 0.7312\n",
      "step: 6300 | acc: 0.7372\n",
      "step: 6400 | acc: 0.7436\n",
      "step: 6500 | acc: 0.7308\n",
      "step: 6600 | acc: 0.7490\n",
      "step: 6700 | acc: 0.7544\n",
      "step: 6800 | acc: 0.7456\n",
      "step: 6900 | acc: 0.7579\n",
      "step: 7000 | acc: 0.7534\n",
      "step: 7100 | acc: 0.7424\n",
      "step: 7200 | acc: 0.7559\n",
      "step: 7300 | acc: 0.7463\n",
      "step: 7400 | acc: 0.7697\n",
      "step: 7500 | acc: 0.7678\n",
      "step: 7600 | acc: 0.7759\n",
      "step: 7700 | acc: 0.7694\n",
      "step: 7800 | acc: 0.7623\n",
      "step: 7900 | acc: 0.7736\n",
      "step: 8000 | acc: 0.7728\n",
      "step: 8100 | acc: 0.7800\n",
      "step: 8200 | acc: 0.7455\n",
      "step: 8300 | acc: 0.7870\n",
      "step: 8400 | acc: 0.7923\n",
      "step: 8500 | acc: 0.7909\n",
      "step: 8600 | acc: 0.7866\n",
      "step: 8700 | acc: 0.7804\n",
      "step: 8800 | acc: 0.7891\n",
      "step: 8900 | acc: 0.7928\n",
      "step: 9000 | acc: 0.7892\n",
      "step: 9100 | acc: 0.7983\n",
      "step: 9200 | acc: 0.7962\n",
      "step: 9300 | acc: 0.8005\n",
      "step: 9400 | acc: 0.8025\n",
      "step: 9500 | acc: 0.7898\n",
      "step: 9600 | acc: 0.8056\n",
      "step: 9700 | acc: 0.8075\n",
      "step: 9800 | acc: 0.8079\n",
      "step: 9900 | acc: 0.8065\n"
     ]
    }
   ],
   "source": [
    "def train_mnist():\n",
    "    import datasets\n",
    "\n",
    "    mnist = datasets.load_dataset(\"mnist\")\n",
    "    xtrain, ytrain = np.array(mnist[\"train\"][\"image\"]).reshape(-1, 784) / 255.0, mnist[\"train\"][\"label\"]\n",
    "    xtest, ytest = np.array(mnist[\"test\"][\"image\"]).reshape(-1, 784) / 255.0, mnist[\"test\"][\"label\"]\n",
    "\n",
    "    def compute_val_acc(params):\n",
    "        val_correct = 0\n",
    "        for x, y in zip(xtest, ytest):\n",
    "            z, _ = forward(x, params)\n",
    "            val_correct += np.argmax(z) == y\n",
    "        return val_correct / len(xtest)\n",
    "\n",
    "    params = init_params(784, 20, [32, 16])\n",
    "    lr = 1e-3\n",
    "    n_examples = 10000\n",
    "    log_every_n_steps = 100\n",
    "\n",
    "    for step in range(n_examples):\n",
    "        # get random training example\n",
    "        i = np.random.randint(len(xtrain))\n",
    "        x, y = xtrain[i], ytrain[i]\n",
    "\n",
    "        # compute gradient\n",
    "        grad = backprop(x, y, params)\n",
    "\n",
    "        # update the parameters\n",
    "        for k in range(len(params)):\n",
    "            params[k][0] -= lr * grad[k][0] \n",
    "            params[k][1] -= lr * grad[k][1]\n",
    "\n",
    "        # log\n",
    "        if step % log_every_n_steps == 0:\n",
    "            print(f\"step: {step} | acc: {compute_val_acc(params):.4f}\")\n",
    "\n",
    "train_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifying Backprop\n",
    "\n",
    "We can verify that our backprop gives a correct by comparing it with a numerically computed approximation of the gradient, which we can obtain via (using a small value for $h$):\n",
    "\n",
    "$$\n",
    "f'(x) = \\lim_{h\\rightarrow 0} \\frac{f(x - h) - f(x + h)}{2h}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Difference Norm: 4.874888229837376e-09\n",
      "Mean Difference Norm: 1.989803113430697e-09\n",
      "\n",
      "Max Relative Diff: 0.00035643038229495977\n",
      "Mean Relative Diff: 7.079203981904641e-08\n",
      "\n",
      "gradient checking passed ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "def numerical_grad(x, y, params, h=1e-6):\n",
    "    compute_loss = lambda params: loss_fn(forward(x, params)[0], y)  # noqa: E731\n",
    "\n",
    "    grad = copy.deepcopy(params)\n",
    "    for i in range(len(params)):\n",
    "        for j in range(len(params[i])):\n",
    "            for k in np.ndindex(params[i][j].shape):\n",
    "                prev_value = params[i][j][k]\n",
    "                params[i][j][k] += h\n",
    "                l = compute_loss(params)\n",
    "                params[i][j][k] -= 2*h\n",
    "                r = compute_loss(params)\n",
    "                grad[i][j][k] = (l - r) / (2 * h)\n",
    "                params[i][j][k] = prev_value\n",
    "\n",
    "    return grad\n",
    "\n",
    "def check_gradients():\n",
    "    x, y = np.random.randn(256), np.random.randint(4)\n",
    "\n",
    "    params = init_params(256, 4, [128, 64, 32, 16, 8])\n",
    "    agrad = backprop(x, y, params)\n",
    "    ngrad = numerical_grad(x, y, params)\n",
    "\n",
    "    diff = []\n",
    "    rels = []\n",
    "    for (aw, ab), (bw, bb) in zip(agrad, ngrad):\n",
    "        diff.append(np.linalg.norm(aw - bw) / (np.linalg.norm(aw) + np.linalg.norm(bw)))\n",
    "        diff.append(np.linalg.norm(ab - bb) / (np.linalg.norm(ab) + np.linalg.norm(bb)))\n",
    "        \n",
    "        rels += (abs(aw - bw) / (abs(aw) + np.finfo(aw.dtype).smallest_subnormal)).flatten().tolist()\n",
    "        rels += (abs(ab - bb) / (abs(bb) + np.finfo(aw.dtype).smallest_subnormal)).flatten().tolist()\n",
    "    \n",
    "    diff = np.array(diff)\n",
    "    rels = np.array(rels)\n",
    "    \n",
    "    print(\"Max Difference Norm:\", diff.max())\n",
    "    print(\"Mean Difference Norm:\", diff.mean())\n",
    "    print()\n",
    "    print(\"Max Relative Diff:\", rels.max())\n",
    "    print(\"Mean Relative Diff:\", rels.mean())\n",
    "    print()\n",
    "\n",
    "    assert diff.max() < 1e-6\n",
    "    assert rels.max() < 0.05\n",
    "    print(\"gradient checking passed ✅\")\n",
    "    print()\n",
    "\n",
    "check_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
