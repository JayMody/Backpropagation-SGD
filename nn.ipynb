{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./.venv/lib/python3.11/site-packages (1.26.2)\n",
      "Requirement already satisfied: datasets in ./.venv/lib/python3.11/site-packages (2.16.0)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.11/site-packages (10.1.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.11/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./.venv/lib/python3.11/site-packages (from datasets) (14.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./.venv/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in ./.venv/lib/python3.11/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.11/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.venv/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./.venv/lib/python3.11/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in ./.venv/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in ./.venv/lib/python3.11/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in ./.venv/lib/python3.11/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in ./.venv/lib/python3.11/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in ./.venv/lib/python3.11/site-packages (from datasets) (0.20.1)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.venv/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.venv/lib/python3.11/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy datasets Pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return x / np.sum(x, axis=-1, keepdims=True)\n",
    "\n",
    "def log_softmax(x):\n",
    "    x = x - np.max(x, axis=-1, keepdims=True)\n",
    "    return x - np.log(np.sum(np.exp(x), axis=-1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(logits, y):\n",
    "    return -log_softmax(logits)[y]\n",
    "\n",
    "def d_loss_fn(logits, y):\n",
    "    p = softmax(logits)\n",
    "    p = p - np.eye(logits.shape[-1])[y]\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def d_relu(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(in_size: int, out_size: int, hidden_sizes: list[int]):\n",
    "    dims = [in_size] + hidden_sizes + [out_size]\n",
    "    params = []\n",
    "    for nin, nout in zip(dims[:-1], dims[1:]):\n",
    "        # xavier uniform initialization for weight matrixes\n",
    "        a = (6 / (nin + nout)) ** 0.5\n",
    "        w = np.random.uniform(-a, a, size=[nin, nout])\n",
    "        b = np.zeros(nout)\n",
    "        params.append([w, b])\n",
    "    return params\n",
    "\n",
    "def forward(x, params):\n",
    "    tape = [(None, x)]\n",
    "    for w, b in params:\n",
    "        z = x @ w + b\n",
    "        x = relu(z)\n",
    "        tape.append((z, x))\n",
    "    return z, tape[:-1]\n",
    "\n",
    "def backprop(x, y, params):\n",
    "    logits, tape = forward(x, params)\n",
    "\n",
    "    grad = []\n",
    "    error = d_loss_fn(logits, y)\n",
    "    for (z, a), (w, _) in zip(reversed(tape), reversed(params)):\n",
    "        grad_w = np.sum(error[...,np.newaxis,:] * a[...,:,np.newaxis], axis=0) / x.shape[0]\n",
    "        grad_b = np.sum(error, axis=0) / x.shape[0]\n",
    "        grad.append((grad_w, grad_b))\n",
    "        if z is not None:\n",
    "            error = error @ w.T\n",
    "            error = error * d_relu(z)\n",
    "    grad = list(reversed(grad))\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jay/code/backpropagation/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 | step: 0 | acc: 0.0547\n",
      "epoch: 0 | step: 100 | acc: 0.0883\n",
      "epoch: 0 | step: 200 | acc: 0.1427\n",
      "epoch: 0 | step: 300 | acc: 0.2094\n",
      "epoch: 0 | step: 400 | acc: 0.2627\n",
      "epoch: 0 | step: 500 | acc: 0.3055\n",
      "epoch: 0 | step: 600 | acc: 0.3634\n",
      "epoch: 0 | step: 700 | acc: 0.4181\n",
      "epoch: 0 | step: 800 | acc: 0.4804\n",
      "epoch: 0 | step: 900 | acc: 0.5387\n",
      "epoch: 1 | step: 0 | acc: 0.5559\n",
      "epoch: 1 | step: 100 | acc: 0.5906\n",
      "epoch: 1 | step: 200 | acc: 0.6230\n",
      "epoch: 1 | step: 300 | acc: 0.6514\n",
      "epoch: 1 | step: 400 | acc: 0.6807\n",
      "epoch: 1 | step: 500 | acc: 0.7035\n",
      "epoch: 1 | step: 600 | acc: 0.7250\n",
      "epoch: 1 | step: 700 | acc: 0.7381\n",
      "epoch: 1 | step: 800 | acc: 0.7479\n",
      "epoch: 1 | step: 900 | acc: 0.7592\n",
      "epoch: 2 | step: 0 | acc: 0.7635\n",
      "epoch: 2 | step: 100 | acc: 0.7697\n",
      "epoch: 2 | step: 200 | acc: 0.7773\n",
      "epoch: 2 | step: 300 | acc: 0.7854\n",
      "epoch: 2 | step: 400 | acc: 0.7950\n",
      "epoch: 2 | step: 500 | acc: 0.8025\n",
      "epoch: 2 | step: 600 | acc: 0.8037\n",
      "epoch: 2 | step: 700 | acc: 0.8057\n",
      "epoch: 2 | step: 800 | acc: 0.8102\n",
      "epoch: 2 | step: 900 | acc: 0.8132\n",
      "epoch: 3 | step: 0 | acc: 0.8139\n",
      "epoch: 3 | step: 100 | acc: 0.8143\n",
      "epoch: 3 | step: 200 | acc: 0.8186\n",
      "epoch: 3 | step: 300 | acc: 0.8212\n",
      "epoch: 3 | step: 400 | acc: 0.8260\n",
      "epoch: 3 | step: 500 | acc: 0.8308\n",
      "epoch: 3 | step: 600 | acc: 0.8310\n",
      "epoch: 3 | step: 700 | acc: 0.8337\n",
      "epoch: 3 | step: 800 | acc: 0.8350\n",
      "epoch: 3 | step: 900 | acc: 0.8354\n",
      "epoch: 4 | step: 0 | acc: 0.8370\n",
      "epoch: 4 | step: 100 | acc: 0.8380\n",
      "epoch: 4 | step: 200 | acc: 0.8396\n",
      "epoch: 4 | step: 300 | acc: 0.8422\n",
      "epoch: 4 | step: 400 | acc: 0.8450\n",
      "epoch: 4 | step: 500 | acc: 0.8481\n",
      "epoch: 4 | step: 600 | acc: 0.8490\n",
      "epoch: 4 | step: 700 | acc: 0.8485\n",
      "epoch: 4 | step: 800 | acc: 0.8501\n",
      "epoch: 4 | step: 900 | acc: 0.8507\n",
      "epoch: 5 | step: 0 | acc: 0.8504\n",
      "epoch: 5 | step: 100 | acc: 0.8508\n",
      "epoch: 5 | step: 200 | acc: 0.8520\n",
      "epoch: 5 | step: 300 | acc: 0.8539\n",
      "epoch: 5 | step: 400 | acc: 0.8553\n",
      "epoch: 5 | step: 500 | acc: 0.8583\n",
      "epoch: 5 | step: 600 | acc: 0.8582\n",
      "epoch: 5 | step: 700 | acc: 0.8590\n",
      "epoch: 5 | step: 800 | acc: 0.8601\n",
      "epoch: 5 | step: 900 | acc: 0.8607\n",
      "epoch: 6 | step: 0 | acc: 0.8609\n",
      "epoch: 6 | step: 100 | acc: 0.8612\n",
      "epoch: 6 | step: 200 | acc: 0.8614\n",
      "epoch: 6 | step: 300 | acc: 0.8626\n",
      "epoch: 6 | step: 400 | acc: 0.8652\n",
      "epoch: 6 | step: 500 | acc: 0.8670\n",
      "epoch: 6 | step: 600 | acc: 0.8672\n",
      "epoch: 6 | step: 700 | acc: 0.8662\n",
      "epoch: 6 | step: 800 | acc: 0.8687\n",
      "epoch: 6 | step: 900 | acc: 0.8691\n",
      "epoch: 7 | step: 0 | acc: 0.8685\n",
      "epoch: 7 | step: 100 | acc: 0.8696\n",
      "epoch: 7 | step: 200 | acc: 0.8690\n",
      "epoch: 7 | step: 300 | acc: 0.8699\n",
      "epoch: 7 | step: 400 | acc: 0.8715\n",
      "epoch: 7 | step: 500 | acc: 0.8724\n",
      "epoch: 7 | step: 600 | acc: 0.8724\n",
      "epoch: 7 | step: 700 | acc: 0.8724\n",
      "epoch: 7 | step: 800 | acc: 0.8744\n",
      "epoch: 7 | step: 900 | acc: 0.8754\n",
      "epoch: 8 | step: 0 | acc: 0.8744\n",
      "epoch: 8 | step: 100 | acc: 0.8745\n",
      "epoch: 8 | step: 200 | acc: 0.8738\n",
      "epoch: 8 | step: 300 | acc: 0.8745\n",
      "epoch: 8 | step: 400 | acc: 0.8753\n",
      "epoch: 8 | step: 500 | acc: 0.8773\n",
      "epoch: 8 | step: 600 | acc: 0.8774\n",
      "epoch: 8 | step: 700 | acc: 0.8771\n",
      "epoch: 8 | step: 800 | acc: 0.8800\n",
      "epoch: 8 | step: 900 | acc: 0.8797\n",
      "epoch: 9 | step: 0 | acc: 0.8799\n",
      "epoch: 9 | step: 100 | acc: 0.8787\n",
      "epoch: 9 | step: 200 | acc: 0.8787\n",
      "epoch: 9 | step: 300 | acc: 0.8788\n",
      "epoch: 9 | step: 400 | acc: 0.8800\n",
      "epoch: 9 | step: 500 | acc: 0.8807\n",
      "epoch: 9 | step: 600 | acc: 0.8813\n",
      "epoch: 9 | step: 700 | acc: 0.8812\n",
      "epoch: 9 | step: 800 | acc: 0.8826\n",
      "epoch: 9 | step: 900 | acc: 0.8826\n"
     ]
    }
   ],
   "source": [
    "def train_mnist():\n",
    "    import datasets\n",
    "\n",
    "    mnist = datasets.load_dataset(\"mnist\")\n",
    "    xtrain, ytrain = np.array(mnist[\"train\"][\"image\"]).reshape(-1, 784) / 255.0, mnist[\"train\"][\"label\"]\n",
    "    xtest, ytest = np.array(mnist[\"test\"][\"image\"]).reshape(-1, 784) / 255.0, mnist[\"test\"][\"label\"]\n",
    "\n",
    "    def compute_val_acc(params):\n",
    "        val_correct = 0\n",
    "        for x, y in zip(xtest, ytest):\n",
    "            z, _ = forward(x, params)\n",
    "            val_correct += np.argmax(z) == y\n",
    "        return val_correct / len(xtest)\n",
    "\n",
    "    lr = 1e-3\n",
    "    bs = 64\n",
    "    n_epochs = 10\n",
    "    log_every_n_steps = 100\n",
    "\n",
    "    params = init_params(784, 20, [64])\n",
    "    for epoch in range(n_epochs):\n",
    "        for step, idx in enumerate(range(0, len(xtrain), bs)):\n",
    "            # get batch of training examples\n",
    "            x, y = xtrain[idx:idx+bs], ytrain[idx:idx+bs]\n",
    "\n",
    "            # compute gradient\n",
    "            grad = backprop(x, y, params)\n",
    "\n",
    "            # update the parameters\n",
    "            for k in range(len(params)):\n",
    "                params[k][0] -= lr * grad[k][0] \n",
    "                params[k][1] -= lr * grad[k][1]\n",
    "\n",
    "            # log\n",
    "            if step % log_every_n_steps == 0:\n",
    "                print(f\"epoch: {epoch} | step: {step} | acc: {compute_val_acc(params):.4f}\")\n",
    "\n",
    "train_mnist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifying Backprop\n",
    "\n",
    "We can verify that our backprop gives a correct by comparing it with a numerically computed approximation of the gradient, which we can obtain via (using a small value for $h$):\n",
    "\n",
    "$$\n",
    "f'(x) = \\lim_{h\\rightarrow 0} \\frac{f(x - h) - f(x + h)}{2h}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Difference Norm: 3.4893667378829324e-09\n",
      "Mean Difference Norm: 1.2138554306040649e-09\n",
      "\n",
      "Max Relative Diff: 5.8688532911883646e-05\n",
      "Mean Relative Diff: 2.8477597187627406e-08\n",
      "\n",
      "gradient checking passed ✅\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "def numerical_grad(x, y, params, h=1e-6):\n",
    "    compute_loss = lambda params: loss_fn(forward(x, params)[0], y)  # noqa: E731\n",
    "\n",
    "    grad = copy.deepcopy(params)\n",
    "    for i in range(len(params)):\n",
    "        for j in range(len(params[i])):\n",
    "            for k in np.ndindex(params[i][j].shape):\n",
    "                prev_value = params[i][j][k]\n",
    "                params[i][j][k] += h\n",
    "                l = compute_loss(params)\n",
    "                params[i][j][k] -= 2*h\n",
    "                r = compute_loss(params)\n",
    "                grad[i][j][k] = (l - r) / (2 * h)\n",
    "                params[i][j][k] = prev_value\n",
    "\n",
    "    return grad\n",
    "\n",
    "def check_gradients():\n",
    "    x, y = np.random.randn(256), np.random.randint(4)\n",
    "\n",
    "    params = init_params(256, 4, [128, 64, 32, 16, 8])\n",
    "    agrad = backprop(x.reshape(1, -1), y, params)\n",
    "    ngrad = numerical_grad(x, y, params)\n",
    "\n",
    "    diff = []\n",
    "    rels = []\n",
    "    for (aw, ab), (bw, bb) in zip(agrad, ngrad):\n",
    "        diff.append(np.linalg.norm(aw - bw) / (np.linalg.norm(aw) + np.linalg.norm(bw)))\n",
    "        diff.append(np.linalg.norm(ab - bb) / (np.linalg.norm(ab) + np.linalg.norm(bb)))\n",
    "        \n",
    "        rels += (abs(aw - bw) / (abs(aw) + np.finfo(aw.dtype).smallest_subnormal)).flatten().tolist()\n",
    "        rels += (abs(ab - bb) / (abs(bb) + np.finfo(aw.dtype).smallest_subnormal)).flatten().tolist()\n",
    "    \n",
    "    diff = np.array(diff)\n",
    "    rels = np.array(rels)\n",
    "    \n",
    "    print(\"Max Difference Norm:\", diff.max())\n",
    "    print(\"Mean Difference Norm:\", diff.mean())\n",
    "    print()\n",
    "    print(\"Max Relative Diff:\", rels.max())\n",
    "    print(\"Mean Relative Diff:\", rels.mean())\n",
    "    print()\n",
    "\n",
    "    assert diff.max() < 1e-6\n",
    "    assert rels.max() < 0.05\n",
    "    print(\"gradient checking passed ✅\")\n",
    "    print()\n",
    "\n",
    "check_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
